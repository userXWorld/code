"""
HIVé•¿æœŸæ²»ç–—ç­–ç•¥ä¼˜åŒ–ç³»ç»Ÿ
åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ (DQN)çš„ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆç”Ÿæˆ
"""
import sys
import io
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ ‡å‡†è¾“å‡ºä¸ºUTF-8ç¼–ç 
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒæ¨¡æ‹Ÿå™¨ ====================

class HIVTreatmentEnv:
    """
    HIVæ²»ç–—ç¯å¢ƒæ¨¡æ‹Ÿå™¨
    æ¨¡æ‹Ÿæ‚£è€…å…ç–«ç³»ç»Ÿåœ¨ä¸åŒæ²»ç–—æ–¹æ¡ˆä¸‹çš„åŠ¨æ€æ¼”å˜
    """
    def __init__(self, patient_id=0):
        self.patient_id = patient_id
        # çŠ¶æ€ç©ºé—´ï¼š[ç—…æ¯’è½½é‡(log), CD4è®¡æ•°, CD8è®¡æ•°, å…ç–«1, å…ç–«2, æ•ˆåº”å™¨]
        self.state_dim = 6
        # åŠ¨ä½œç©ºé—´ï¼š4ç§è¯ç‰©ç»„åˆ [æ— æ²»ç–—, å•è¯, åŒè¯, ä¸‰è¯è”åˆ]
        self.action_dim = 4
        
        # ä¸´åºŠå‚æ•°ï¼ˆæ¥è‡ªæ–‡çŒ®çš„å…¸å‹å€¼ï¼‰
        self.viral_threshold = 500  # ç—…æ¯’æŠ‘åˆ¶ç›®æ ‡
        self.cd4_danger = 200       # CD4å±é™©é˜ˆå€¼
        self.cd4_healthy = 500      # CD4å¥åº·æ°´å¹³
        
        self.reset()
    
    def reset(self):
        """
        é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
        ä¸ºä½•è¿™ä¹ˆå†™ï¼šåˆå§‹çŠ¶æ€æ¨¡æ‹Ÿæœªæ²»ç–—çš„HIVæ„ŸæŸ“æ‚£è€…å…¸å‹æŒ‡æ ‡
        """
        # åˆå§‹ç—…æ¯’è½½é‡ï¼š10^4 - 10^5 copies/mLï¼ˆæœªæ²»ç–—æ‚£è€…å…¸å‹å€¼ï¼‰
        initial_viral = np.random.uniform(4.0, 5.0)  # log10å°ºåº¦
        # åˆå§‹CD4ï¼š200-500 cells/Î¼Lï¼ˆæ„ŸæŸ“ä½†æœªä¸¥é‡å…ç–«æŠ‘åˆ¶ï¼‰
        initial_cd4 = np.random.uniform(200, 500)
        # å…¶ä»–å…ç–«æŒ‡æ ‡éšæœºåˆå§‹åŒ–
        self.state = np.array([
            initial_viral,
            initial_cd4,
            np.random.uniform(800, 1200),  # CD8è®¡æ•°
            np.random.uniform(0.1, 0.3),   # å…ç–«æ•ˆåº”1
            np.random.uniform(0.1, 0.3),   # å…ç–«æ•ˆåº”2
            np.random.uniform(0.05, 0.15)  # æ•ˆåº”å™¨ç»†èƒ
        ])
        self.week = 0
        return self.state.copy()
    
    def step(self, action):
        """
        æ‰§è¡Œæ²»ç–—åŠ¨ä½œï¼Œè¿”å›æ–°çŠ¶æ€å’Œå¥–åŠ±
        
        å‚æ•°ï¼š
            action: 0=æ— æ²»ç–—, 1=å•è¯, 2=åŒè¯, 3=ä¸‰è¯è”åˆ
        
        ä¸ºä½•è¿™ä¹ˆå†™ï¼šä½¿ç”¨ç®€åŒ–çš„HIVåŠ¨åŠ›å­¦æ¨¡å‹ï¼ˆåŸºäºPerelsonæ¨¡å‹ï¼‰
        çœŸå®Health Gymä½¿ç”¨æ›´å¤æ‚çš„å¾®åˆ†æ–¹ç¨‹ï¼Œè¿™é‡Œç”¨è¿‘ä¼¼æ›´æ–°è§„åˆ™
        """
        # è¯ç‰©æ•ˆåŠ›ç³»æ•°
        drug_efficacy = [0.0, 0.5, 0.75, 0.9][action]  # è¶Šå¤šè¯ç‰©ï¼Œæ•ˆåŠ›è¶Šå¼º
        
        # 1. ç—…æ¯’è½½é‡æ›´æ–°ï¼ˆæŒ‡æ•°è¡°å‡ + å¤åˆ¶ï¼‰
        viral_decay = drug_efficacy * 0.5  # æ²»ç–—å¯¼è‡´çš„ç—…æ¯’è¡°å‡
        viral_replication = (1 - drug_efficacy) * 0.3  # æ®‹ä½™å¤åˆ¶
        self.state[0] += -viral_decay + viral_replication + np.random.normal(0, 0.1)
        self.state[0] = np.clip(self.state[0], 1.0, 6.0)  # é™åˆ¶åœ¨åˆç†èŒƒå›´
        
        # 2. CD4è®¡æ•°æ›´æ–°ï¼ˆå—ç—…æ¯’è½½é‡å’Œæ²»ç–—å½±å“ï¼‰
        viral_damage = -0.01 * (10 ** self.state[0]) / 10000  # ç—…æ¯’æ€ä¼¤CD4
        treatment_benefit = drug_efficacy * 5  # æ²»ç–—ä¿ƒè¿›CD4æ¢å¤
        self.state[1] += treatment_benefit + viral_damage + np.random.normal(0, 10)
        self.state[1] = np.clip(self.state[1], 50, 1500)
        
        # 3. å…¶ä»–å…ç–«æŒ‡æ ‡æ›´æ–°ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        self.state[2:] += np.random.normal(0, 0.05, size=4)  # éšæœºæ³¢åŠ¨
        self.state[2:] = np.clip(self.state[2:], 0, 2)
        
        self.week += 1
        
        # è®¡ç®—å¥–åŠ±
        reward = self._compute_reward(action)
        
        # åˆ¤æ–­æ˜¯å¦ç»“æŸï¼ˆ96å‘¨æˆ–CD4è¿‡ä½ï¼‰
        done = (self.week >= 96) or (self.state[1] < 50)
        
        info = {
            'viral_load': 10 ** self.state[0],  # è½¬å›çœŸå®å°ºåº¦
            'cd4_count': self.state[1],
            'week': self.week
        }
        
        return self.state.copy(), reward, done, info
    
    def _compute_reward(self, action):
        """
        è®¡ç®—ç»¼åˆå¥–åŠ±
        ä¸ºä½•è¿™ä¹ˆå†™ï¼šå¤šç›®æ ‡ä¼˜åŒ–éœ€è¦åŠ æƒç»„åˆï¼Œæƒé‡åŸºäºä¸´åºŠä¼˜å…ˆçº§
        """
        # 1. ç—…æ¯’æŠ‘åˆ¶å¥–åŠ±ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ 40%ï¼‰
        viral_log = self.state[0]
        if 10**viral_log < 50:  # å®Œå…¨æŠ‘åˆ¶
            viral_reward = 10.0
        elif 10**viral_log < self.viral_threshold:  # è¾¾æ ‡
            viral_reward = 5.0
        elif 10**viral_log < 10000:  # å¯æ¥å—
            viral_reward = 0.0
        else:  # å¤±è´¥
            viral_reward = -5.0
        
        # 2. CD4ç»´æŒå¥–åŠ±ï¼ˆæ¬¡ä¼˜å…ˆçº§ 30%ï¼‰
        cd4 = self.state[1]
        if cd4 >= self.cd4_healthy:
            cd4_reward = 10.0
        elif cd4 >= 350:
            cd4_reward = 5.0
        elif cd4 >= self.cd4_danger:
            cd4_reward = 0.0
        else:  # å±é™©åŒºåŸŸ
            cd4_reward = -10.0
        
        # 3. ç¨³å®šæ€§å¥–åŠ±ï¼ˆ20%ï¼‰- æƒ©ç½šå‰§çƒˆæ³¢åŠ¨
        # ä¸ºä½•ç”¨getattrï¼šé¦–æ¬¡è°ƒç”¨æ—¶prev_stateä¸å­˜åœ¨ï¼Œé¿å…æŠ¥é”™
        prev_state = getattr(self, 'prev_state', self.state)
        viral_change_rate = abs(self.state[0] - prev_state[0])
        cd4_change_rate = abs(self.state[1] - prev_state[1]) / (prev_state[1] + 1e-6)
        
        if viral_change_rate > 0.5 or cd4_change_rate > 0.3:
            stability_reward = -5.0  # å¤§å¹…æ³¢åŠ¨
        elif viral_change_rate > 0.2 or cd4_change_rate > 0.15:
            stability_reward = -2.0  # ä¸­åº¦æ³¢åŠ¨
        else:
            stability_reward = 2.0   # ç¨³å®š
        
        self.prev_state = self.state.copy()
        
        # 4. æ²»ç–—è´Ÿæ‹…æƒ©ç½šï¼ˆ10%ï¼‰
        treatment_burden = -action * 0.5  # è¶Šå¤šè¯ç‰©ï¼Œè´Ÿæ‹…è¶Šå¤§
        
        # é¢å¤–æƒ©ç½šé¢‘ç¹åˆ‡æ¢
        prev_action = getattr(self, 'prev_action', action)
        if action != prev_action and self.week > 0:
            treatment_burden -= 2.0  # åˆ‡æ¢æˆæœ¬
        self.prev_action = action
        
        # åŠ æƒç»„åˆï¼ˆæ€»å’Œä¸º1.0ï¼Œç¡®ä¿å¯æ¯”æ€§ï¼‰
        total_reward = (
            0.4 * viral_reward +
            0.3 * cd4_reward +
            0.2 * stability_reward +
            0.1 * treatment_burden
        )
        
        return total_reward


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæ·±åº¦Qç½‘ç»œ ====================

class DQN(nn.Module):
    """
    Deep Q-Network
    ä¸ºä½•ç”¨è¿™ä¸ªç»“æ„ï¼š
    - ä¸¤å±‚éšè—å±‚è¶³å¤Ÿæ‹ŸåˆHIVåŠ¨åŠ›å­¦
    - Dropouté˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆæ•°æ®é‡å¯èƒ½æœ‰é™ï¼‰
    - ReLUæ¿€æ´»é¿å…æ¢¯åº¦æ¶ˆå¤±
    """
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super(DQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),  # 20%dropoutç‡ï¼Œå¹³è¡¡æ­£åˆ™åŒ–å’Œå®¹é‡
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, action_dim)
        )
    
    def forward(self, x):
        return self.network(x)


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ ====================

class HIVTreatmentAgent:
    """
    DQNæ™ºèƒ½ä½“ï¼Œå­¦ä¹ æœ€ä¼˜æ²»ç–—ç­–ç•¥
    """
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # è®¾å¤‡é€‰æ‹©ï¼ˆä¼˜å…ˆGPUï¼‰
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŒç½‘ç»œæ¶æ„ï¼ˆç­–ç•¥ç½‘ç»œ + ç›®æ ‡ç½‘ç»œï¼‰
        # ä¸ºä½•ç”¨åŒç½‘ç»œï¼šç¨³å®šè®­ç»ƒï¼Œé¿å…Qå€¼ä¼°è®¡éœ‡è¡
        self.policy_net = DQN(state_dim, action_dim).to(self.device)
        self.target_net = DQN(state_dim, action_dim).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()  # ç›®æ ‡ç½‘ç»œä¸å‚ä¸è®­ç»ƒ
        
        # ä¼˜åŒ–å™¨ï¼ˆAdamè‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚åˆéå¹³ç¨³é—®é¢˜ï¼‰
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆæ‰“ç ´æ•°æ®ç›¸å…³æ€§ï¼‰
        # ä¸ºä½•ç”¨dequeï¼šè‡ªåŠ¨ä¸¢å¼ƒæ—§æ•°æ®ï¼Œä¿æŒå›ºå®šå®¹é‡
        self.memory = deque(maxlen=10000)
        
        # è¶…å‚æ•°
        self.batch_size = 64
        self.gamma = 0.99  # æŠ˜æ‰£å› å­ï¼Œæ¥è¿‘1é‡è§†é•¿æœŸå›æŠ¥
        self.epsilon = 1.0  # åˆå§‹æ¢ç´¢ç‡
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.target_update_freq = 10  # æ¯10è½®æ›´æ–°ç›®æ ‡ç½‘ç»œ
    
    def select_action(self, state, training=True):
        """
        Îµ-è´ªå¿ƒç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        ä¸ºä½•è¿™ä¹ˆå†™ï¼šå¹³è¡¡æ¢ç´¢(éšæœº)å’Œåˆ©ç”¨(æœ€ä¼˜)
        """
        if training and random.random() < self.epsilon:
            return random.randint(0, self.action_dim - 1)  # æ¢ç´¢
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.policy_net(state_tensor)
                return q_values.argmax().item()  # åˆ©ç”¨
    
    def store_transition(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.memory.append((state, action, reward, next_state, done))
    
    def train_step(self):
        """
        ä»ç»éªŒå›æ”¾ä¸­é‡‡æ ·å¹¶è®­ç»ƒ
        ä¸ºä½•æ‰¹é‡è®­ç»ƒï¼šæé«˜æ ·æœ¬åˆ©ç”¨ç‡ï¼Œç¨³å®šæ¢¯åº¦
        """
        if len(self.memory) < self.batch_size:
            return 0.0  # æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡
        
        # éšæœºé‡‡æ ·batchï¼ˆæ‰“ç ´æ—¶é—´ç›¸å…³æ€§ï¼‰
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        # è½¬æ¢ä¸ºå¼ é‡ï¼ˆä¸ºä½•ç”¨stackï¼šä¿æŒbatchç»´åº¦ï¼‰
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))
        
        # è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆBellmanæ–¹ç¨‹ï¼‰
        with torch.no_grad():
            next_q = self.target_net(next_states).max(1)[0]
            # ä¸ºä½•ä¹˜(1-dones)ï¼šç»ˆæ­¢çŠ¶æ€æ— æœªæ¥å›æŠ¥
            target_q = rewards + self.gamma * next_q * (1 - dones)
        
        # MSEæŸå¤±ï¼ˆä¸ºä½•ç”¨MSEï¼šQå€¼å›å½’é—®é¢˜ï¼‰
        loss = nn.MSELoss()(current_q.squeeze(), target_q)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ªï¼ˆä¸ºä½•éœ€è¦ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def update_target_network(self):
        """å°†ç­–ç•¥ç½‘ç»œæƒé‡å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œ"""
        self.target_net.load_state_dict(self.policy_net.state_dict())
    
    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒæµç¨‹ ====================

def train_agent(env, agent, num_episodes=500):
    """
    è®­ç»ƒæ™ºèƒ½ä½“
    è¿”å›ï¼šè®­ç»ƒå†å²è®°å½•
    """
    history = {
        'episode_rewards': [],
        'viral_suppression_rates': [],
        'cd4_safety_rates': [],
        'losses': [],
        'epsilons': []
    }
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    for episode in tqdm(range(num_episodes), desc="è®­ç»ƒè¿›åº¦"):
        state = env.reset()
        episode_reward = 0
        episode_losses = []
        viral_suppressed = 0
        cd4_safe = 0
        
        for week in range(96):
            # é€‰æ‹©å¹¶æ‰§è¡ŒåŠ¨ä½œ
            action = agent.select_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            agent.store_transition(state, action, reward, next_state, done)
            
            # è®­ç»ƒ
            loss = agent.train_step()
            if loss > 0:
                episode_losses.append(loss)
            
            # ç»Ÿè®¡æŒ‡æ ‡
            episode_reward += reward
            if info['viral_load'] < 500:
                viral_suppressed += 1
            if info['cd4_count'] >= 200:
                cd4_safe += 1
            
            state = next_state
            if done:
                break
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % agent.target_update_freq == 0:
            agent.update_target_network()
        
        # è¡°å‡æ¢ç´¢ç‡
        agent.decay_epsilon()
        
        # è®°å½•å†å²
        history['episode_rewards'].append(episode_reward)
        history['viral_suppression_rates'].append(viral_suppressed / 96)
        history['cd4_safety_rates'].append(cd4_safe / 96)
        history['losses'].append(np.mean(episode_losses) if episode_losses else 0)
        history['epsilons'].append(agent.epsilon)
        
        # æ¯50è½®æ‰“å°è¿›åº¦
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(history['episode_rewards'][-50:])
            avg_viral_supp = np.mean(history['viral_suppression_rates'][-50:])
            print(f"\nğŸ“Š Episode {episode+1}: "
                  f"å¹³å‡å¥–åŠ±={avg_reward:.2f}, "
                  f"ç—…æ¯’æŠ‘åˆ¶ç‡={avg_viral_supp*100:.1f}%, "
                  f"Îµ={agent.epsilon:.3f}")
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    return history


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šè¯„ä¼°ä¸å¯¹æ¯” ====================

def evaluate_policy(env, agent, num_episodes=100):
    """
    è¯„ä¼°è®­ç»ƒå¥½çš„ç­–ç•¥ï¼ˆæ— æ¢ç´¢ï¼‰
    """
    results = {
        'episode_rewards': [],
        'viral_loads': [],
        'cd4_counts': [],
        'actions': [],
        'viral_suppression_rates': [],
        'cd4_safety_rates': [],
        'treatment_switches': []
    }
    
    print("\nğŸ” è¯„ä¼°ç­–ç•¥æ€§èƒ½...")
    for episode in tqdm(range(num_episodes), desc="è¯„ä¼°è¿›åº¦"):
        state = env.reset()
        episode_data = {
            'rewards': [],
            'viral_loads': [],
            'cd4_counts': [],
            'actions': []
        }
        viral_suppressed = 0
        cd4_safe = 0
        switches = 0
        prev_action = None
        
        for week in range(96):
            action = agent.select_action(state, training=False)  # æ— æ¢ç´¢
            next_state, reward, done, info = env.step(action)
            
            episode_data['rewards'].append(reward)
            episode_data['viral_loads'].append(info['viral_load'])
            episode_data['cd4_counts'].append(info['cd4_count'])
            episode_data['actions'].append(action)
            
            if info['viral_load'] < 500:
                viral_suppressed += 1
            if info['cd4_count'] >= 200:
                cd4_safe += 1
            if prev_action is not None and action != prev_action:
                switches += 1
            
            prev_action = action
            state = next_state
            if done:
                break
        
        results['episode_rewards'].append(np.sum(episode_data['rewards']))
        results['viral_loads'].append(episode_data['viral_loads'])
        results['cd4_counts'].append(episode_data['cd4_counts'])
        results['actions'].append(episode_data['actions'])
        results['viral_suppression_rates'].append(viral_suppressed / 96)
        results['cd4_safety_rates'].append(cd4_safe / 96)
        results['treatment_switches'].append(switches)
    
    return results


def evaluate_baseline(env, strategy_name, num_episodes=100):
    """
    è¯„ä¼°åŸºçº¿ç­–ç•¥
    strategy_name: 'fixed' (å›ºå®šæ–¹æ¡ˆ) æˆ– 'cycling' (å¾ªç¯æ–¹æ¡ˆ)
    """
    results = {
        'episode_rewards': [],
        'viral_suppression_rates': [],
        'cd4_safety_rates': [],
        'treatment_switches': []
    }
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        viral_suppressed = 0
        cd4_safe = 0
        switches = 0
        prev_action = None
        
        for week in range(96):
            # åŸºçº¿ç­–ç•¥é€‰æ‹©
            if strategy_name == 'fixed':
                action = 3  # å›ºå®šä½¿ç”¨ä¸‰è¯è”åˆ
            elif strategy_name == 'cycling':
                action = week % 4  # æ¯å‘¨å¾ªç¯åˆ‡æ¢
            
            next_state, reward, done, info = env.step(action)
            
            episode_reward += reward
            if info['viral_load'] < 500:
                viral_suppressed += 1
            if info['cd4_count'] >= 200:
                cd4_safe += 1
            if prev_action is not None and action != prev_action:
                switches += 1
            
            prev_action = action
            state = next_state
            if done:
                break
        
        results['episode_rewards'].append(episode_reward)
        results['viral_suppression_rates'].append(viral_suppressed / 96)
        results['cd4_safety_rates'].append(cd4_safe / 96)
        results['treatment_switches'].append(switches)
    
    return results


# ==================== ç¬¬å…­éƒ¨åˆ†ï¼šå¯è§†åŒ–æ¨¡å— ====================

def plot_training_history(history):
    """
    ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿
    ä¸ºä½•ç”¨4å­å›¾ï¼šå…¨é¢å±•ç¤ºè®­ç»ƒåŠ¨æ€ï¼ˆå¥–åŠ±ã€æŒ‡æ ‡ã€æŸå¤±ã€æ¢ç´¢ï¼‰
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. å¥–åŠ±æ›²çº¿ï¼ˆå¹³æ»‘å¤„ç†ï¼‰
    # ä¸ºä½•ç”¨ç§»åŠ¨å¹³å‡ï¼šå‡å°‘å™ªå£°ï¼Œæ›´æ¸…æ™°å±•ç¤ºè¶‹åŠ¿
    window = 20
    smoothed_rewards = pd.Series(history['episode_rewards']).rolling(window).mean()
    axes[0, 0].plot(smoothed_rewards, 'b-', linewidth=2, label='å¹³æ»‘å¥–åŠ±')
    axes[0, 0].plot(history['episode_rewards'], 'b-', alpha=0.3, label='åŸå§‹å¥–åŠ±')
    axes[0, 0].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)
    axes[0, 0].set_ylabel('ç´¯ç§¯å¥–åŠ±', fontsize=12)
    axes[0, 0].set_title('è®­ç»ƒå¥–åŠ±æ›²çº¿', fontsize=14, fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ç—…æ¯’æŠ‘åˆ¶ç‡
    axes[0, 1].plot(history['viral_suppression_rates'], 'g-', linewidth=2)
    axes[0, 1].axhline(y=0.85, color='r', linestyle='--', label='ä¼˜ç§€ç›®æ ‡(85%)')
    axes[0, 1].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)
    axes[0, 1].set_ylabel('ç—…æ¯’æŠ‘åˆ¶ç‡', fontsize=12)
    axes[0, 1].set_title('ç—…æ¯’æŠ‘åˆ¶ç‡æ¼”å˜', fontsize=14, fontweight='bold')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].set_ylim([0, 1])
    
    # 3. è®­ç»ƒæŸå¤±
    axes[1, 0].plot(history['losses'], 'orange', linewidth=2)
    axes[1, 0].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)
    axes[1, 0].set_ylabel('å¹³å‡æŸå¤±', fontsize=12)
    axes[1, 0].set_title('è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_yscale('log')  # å¯¹æ•°åˆ»åº¦æ›´æ¸…æ™°
    
    # 4. æ¢ç´¢ç‡è¡°å‡
    axes[1, 1].plot(history['epsilons'], 'purple', linewidth=2)
    axes[1, 1].set_xlabel('è®­ç»ƒè½®æ¬¡', fontsize=12)
    axes[1, 1].set_ylabel('æ¢ç´¢ç‡ Îµ', fontsize=12)
    axes[1, 1].set_title('æ¢ç´¢ç‡è¡°å‡', fontsize=14, fontweight='bold')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
    print("ğŸ“ˆ è®­ç»ƒå†å²å›¾å·²ä¿å­˜: training_history.png")
    plt.show()


def plot_treatment_trajectory(results, episode_idx=0):
    """
    å¯è§†åŒ–å•ä¸ªæ‚£è€…çš„æ²»ç–—è½¨è¿¹
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    weeks = range(len(results['viral_loads'][episode_idx]))
    viral_loads = results['viral_loads'][episode_idx]
    cd4_counts = results['cd4_counts'][episode_idx]
    actions = results['actions'][episode_idx]
    
    # 1. ç—…æ¯’è½½é‡æ¼”å˜
    axes[0, 0].plot(weeks, viral_loads, 'b-', linewidth=2, marker='o', markersize=3)
    axes[0, 0].axhline(y=500, color='r', linestyle='--', linewidth=2, label='æŠ‘åˆ¶ç›®æ ‡')
    axes[0, 0].axhline(y=50, color='g', linestyle='--', linewidth=2, label='å®Œå…¨æŠ‘åˆ¶')
    axes[0, 0].set_xlabel('å‘¨æ•°', fontsize=12)
    axes[0, 0].set_ylabel('ç—…æ¯’è½½é‡ (copies/mL)', fontsize=12)
    axes[0, 0].set_title('ç—…æ¯’è½½é‡æ¼”å˜', fontsize=14, fontweight='bold')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_yscale('log')  # å¯¹æ•°åˆ»åº¦ï¼ˆç—…æ¯’è½½é‡è·¨åº¦å¤§ï¼‰
    
    # 2. CD4è®¡æ•°æ¼”å˜
    axes[0, 1].plot(weeks, cd4_counts, 'g-', linewidth=2, marker='s', markersize=3)
    axes[0, 1].axhline(y=500, color='g', linestyle='--', linewidth=2, label='å¥åº·æ°´å¹³')
    axes[0, 1].axhline(y=200, color='r', linestyle='--', linewidth=2, label='å±é™©é˜ˆå€¼')
    axes[0, 1].set_xlabel('å‘¨æ•°', fontsize=12)
    axes[0, 1].set_ylabel('CD4è®¡æ•° (cells/Î¼L)', fontsize=12)
    axes[0, 1].set_title('CD4ç»†èƒæ¼”å˜', fontsize=14, fontweight='bold')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æ²»ç–—æ–¹æ¡ˆæ—¶é—´çº¿
    action_names = ['æ— æ²»ç–—', 'å•è¯', 'åŒè¯', 'ä¸‰è¯è”åˆ']
    # ä¸ºä½•ç”¨æ­¥é˜¶å›¾ï¼šæ¸…æ™°æ˜¾ç¤ºç¦»æ•£æ²»ç–—æ–¹æ¡ˆåˆ‡æ¢
    axes[1, 0].step(weeks, actions, 'purple', linewidth=2, where='post')
    axes[1, 0].set_xlabel('å‘¨æ•°', fontsize=12)
    axes[1, 0].set_ylabel('æ²»ç–—æ–¹æ¡ˆ', fontsize=12)
    axes[1, 0].set_title('æ²»ç–—æ–¹æ¡ˆæ—¶é—´çº¿', fontsize=14, fontweight='bold')
    axes[1, 0].set_yticks(range(4))
    axes[1, 0].set_yticklabels(action_names)
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. åŒæŒ‡æ ‡æ•£ç‚¹å›¾ï¼ˆçŠ¶æ€ç©ºé—´è½¨è¿¹ï¼‰
    # ä¸ºä½•ç”¨æ¸å˜è‰²ï¼šæ˜¾ç¤ºæ—¶é—´ç»´åº¦
    scatter = axes[1, 1].scatter(viral_loads, cd4_counts, c=weeks, 
                                  cmap='viridis', s=50, alpha=0.6)
    axes[1, 1].axvline(x=500, color='r', linestyle='--', alpha=0.5)
    axes[1, 1].axhline(y=200, color='r', linestyle='--', alpha=0.5)
    axes[1, 1].set_xlabel('ç—…æ¯’è½½é‡ (log scale)', fontsize=12)
    axes[1, 1].set_ylabel('CD4è®¡æ•°', fontsize=12)
    axes[1, 1].set_title('çŠ¶æ€ç©ºé—´è½¨è¿¹', fontsize=14, fontweight='bold')
    axes[1, 1].set_xscale('log')
    axes[1, 1].grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=axes[1, 1], label='å‘¨æ•°')
    
    plt.tight_layout()
    plt.savefig('treatment_trajectory.png', dpi=300, bbox_inches='tight')
    print("ğŸ“Š æ²»ç–—è½¨è¿¹å›¾å·²ä¿å­˜: treatment_trajectory.png")
    plt.show()


def plot_comparison(our_results, baseline_fixed, baseline_cycling):
    """
    å¯¹æ¯”æˆ‘ä»¬çš„ç­–ç•¥ä¸åŸºçº¿æ–¹æ³•
    ä¸ºä½•ç”¨æŸ±çŠ¶å›¾+è¯¯å·®çº¿ï¼šç›´è§‚æ¯”è¾ƒå¤šä¸ªæŒ‡æ ‡çš„å‡å€¼å’Œæ–¹å·®
    """
    metrics = ['å¹³å‡å¥–åŠ±', 'ç—…æ¯’æŠ‘åˆ¶ç‡(%)', 'CD4å®‰å…¨ç‡(%)', 'æ²»ç–—åˆ‡æ¢æ¬¡æ•°']
    
    our_scores = [
        np.mean(our_results['episode_rewards']),
        np.mean(our_results['viral_suppression_rates']) * 100,
        np.mean(our_results['cd4_safety_rates']) * 100,
        np.mean(our_results['treatment_switches'])
    ]
    
    fixed_scores = [
        np.mean(baseline_fixed['episode_rewards']),
        np.mean(baseline_fixed['viral_suppression_rates']) * 100,
        np.mean(baseline_fixed['cd4_safety_rates']) * 100,
        np.mean(baseline_fixed['treatment_switches'])
    ]
    
    cycling_scores = [
        np.mean(baseline_cycling['episode_rewards']),
        np.mean(baseline_cycling['viral_suppression_rates']) * 100,
        np.mean(baseline_cycling['cd4_safety_rates']) * 100,
        np.mean(baseline_cycling['treatment_switches'])
    ]
    
    # è®¡ç®—æ ‡å‡†è¯¯å·®ï¼ˆä¸ºä½•ç”¨æ ‡å‡†è¯¯ï¼šè¯„ä¼°ç»“æœçš„å¯é æ€§ï¼‰
    our_std = [
        np.std(our_results['episode_rewards']) / np.sqrt(len(our_results['episode_rewards'])),
        np.std(our_results['viral_suppression_rates']) * 100 / np.sqrt(len(our_results['viral_suppression_rates'])),
        np.std(our_results['cd4_safety_rates']) * 100 / np.sqrt(len(our_results['cd4_safety_rates'])),
        np.std(our_results['treatment_switches']) / np.sqrt(len(our_results['treatment_switches']))
    ]
    
    # ç»˜åˆ¶åˆ†ç»„æŸ±çŠ¶å›¾
    x = np.arange(len(metrics))
    width = 0.25
    
    fig, ax = plt.subplots(figsize=(14, 6))
    
    # ä¸ºä½•ç”¨ä¸åŒé¢œè‰²ï¼šåŒºåˆ†ä¸åŒç­–ç•¥
    bars1 = ax.bar(x - width, our_scores, width, label='æˆ‘ä»¬çš„ç­–ç•¥(DQN)', 
                   color='#2ecc71', alpha=0.8, yerr=our_std, capsize=5)
    bars2 = ax.bar(x, fixed_scores, width, label='å›ºå®šä¸‰è¯è”åˆ', 
                   color='#3498db', alpha=0.8)
    bars3 = ax.bar(x + width, cycling_scores, width, label='å¾ªç¯æ–¹æ¡ˆ', 
                   color='#e74c3c', alpha=0.8)
    
    ax.set_xlabel('è¯„ä¼°æŒ‡æ ‡', fontsize=13, fontweight='bold')
    ax.set_ylabel('å¾—åˆ†', fontsize=13, fontweight='bold')
    ax.set_title('æ²»ç–—ç­–ç•¥æ€§èƒ½å¯¹æ¯”', fontsize=15, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics, fontsize=11)
    ax.legend(fontsize=11, loc='upper right')
    ax.grid(True, alpha=0.3, axis='y')
    
    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ•°å€¼
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}',
                    ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig('strategy_comparison.png', dpi=300, bbox_inches='tight')
    print("ğŸ“Š ç­–ç•¥å¯¹æ¯”å›¾å·²ä¿å­˜: strategy_comparison.png")
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“‹ è¯¦ç»†æ€§èƒ½å¯¹æ¯”")
    print("="*60)
    for i, metric in enumerate(metrics):
        print(f"\n{metric}:")
        print(f"  æˆ‘ä»¬çš„ç­–ç•¥: {our_scores[i]:.2f} Â± {our_std[i]:.2f}")
        print(f"  å›ºå®šæ–¹æ¡ˆ:   {fixed_scores[i]:.2f}")
        print(f"  å¾ªç¯æ–¹æ¡ˆ:   {cycling_scores[i]:.2f}")
        
        # è®¡ç®—ç›¸å¯¹æå‡ï¼ˆä¸ºä½•ç”¨ç™¾åˆ†æ¯”ï¼šæ›´ç›´è§‚ï¼‰
        if fixed_scores[i] != 0:
            improvement = (our_scores[i] - fixed_scores[i]) / abs(fixed_scores[i]) * 100
            print(f"  ç›¸å¯¹å›ºå®šæ–¹æ¡ˆæå‡: {improvement:+.1f}%")


def plot_action_distribution(results):
    """
    åˆ†ææ²»ç–—æ–¹æ¡ˆé€‰æ‹©åˆ†å¸ƒ
    ä¸ºä½•éœ€è¦ï¼šäº†è§£ç­–ç•¥çš„æ²»ç–—åå¥½
    """
    # ç»Ÿè®¡æ‰€æœ‰episodeçš„åŠ¨ä½œåˆ†å¸ƒ
    all_actions = []
    for actions in results['actions']:
        all_actions.extend(actions)
    
    action_names = ['æ— æ²»ç–—', 'å•è¯', 'åŒè¯', 'ä¸‰è¯è”åˆ']
    action_counts = [all_actions.count(i) for i in range(4)]
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # 1. é¥¼å›¾
    colors = ['#95a5a6', '#f39c12', '#3498db', '#2ecc71']
    ax1.pie(action_counts, labels=action_names, autopct='%1.1f%%',
            colors=colors, startangle=90, textprops={'fontsize': 11})
    ax1.set_title('æ²»ç–—æ–¹æ¡ˆåˆ†å¸ƒ', fontsize=14, fontweight='bold')
    
    # 2. æŸ±çŠ¶å›¾
    ax2.bar(action_names, action_counts, color=colors, alpha=0.8)
    ax2.set_xlabel('æ²»ç–—æ–¹æ¡ˆ', fontsize=12)
    ax2.set_ylabel('ä½¿ç”¨æ¬¡æ•°', fontsize=12)
    ax2.set_title('æ²»ç–—æ–¹æ¡ˆä½¿ç”¨é¢‘æ¬¡', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')
    
    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ•°å€¼
    for i, (name, count) in enumerate(zip(action_names, action_counts)):
        ax2.text(i, count, str(count), ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('action_distribution.png', dpi=300, bbox_inches='tight')
    print("ğŸ“Š åŠ¨ä½œåˆ†å¸ƒå›¾å·²ä¿å­˜: action_distribution.png")
    plt.show()


def generate_report(our_results, baseline_fixed, baseline_cycling):
    """
    ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
    ä¸ºä½•éœ€è¦ï¼šæä¾›å¯è§£é‡Šçš„é‡åŒ–ç»“æœ
    """
    print("\n" + "="*70)
    print("ğŸ“„ HIVæ²»ç–—ç­–ç•¥ä¼˜åŒ– - ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
    print("="*70)
    
    print("\nã€1. æ•´ä½“æ€§èƒ½æŒ‡æ ‡ã€‘")
    print("-" * 70)
    
    metrics_dict = {
        'å¹³å‡ç´¯ç§¯å¥–åŠ±': [
            np.mean(our_results['episode_rewards']),
            np.mean(baseline_fixed['episode_rewards']),
            np.mean(baseline_cycling['episode_rewards'])
        ],
        'ç—…æ¯’æŠ‘åˆ¶ç‡(%)': [
            np.mean(our_results['viral_suppression_rates']) * 100,
            np.mean(baseline_fixed['viral_suppression_rates']) * 100,
            np.mean(baseline_cycling['viral_suppression_rates']) * 100
        ],
        'CD4å®‰å…¨ç»´æŒç‡(%)': [
            np.mean(our_results['cd4_safety_rates']) * 100,
            np.mean(baseline_fixed['cd4_safety_rates']) * 100,
            np.mean(baseline_cycling['cd4_safety_rates']) * 100
        ],
        'å¹³å‡æ²»ç–—åˆ‡æ¢æ¬¡æ•°': [
            np.mean(our_results['treatment_switches']),
            np.mean(baseline_fixed['treatment_switches']),
            np.mean(baseline_cycling['treatment_switches'])
        ]
    }
    
    df = pd.DataFrame(metrics_dict, index=['DQNç­–ç•¥', 'å›ºå®šæ–¹æ¡ˆ', 'å¾ªç¯æ–¹æ¡ˆ']).T
    print(df.to_string())
    
    print("\nã€2. ä¸´åºŠæ„ä¹‰è§£è¯»ã€‘")
    print("-" * 70)
    
    viral_supp = np.mean(our_results['viral_suppression_rates']) * 100
    cd4_safe = np.mean(our_results['cd4_safety_rates']) * 100
    switches = np.mean(our_results['treatment_switches'])
    
    if viral_supp >= 85:
        print(f"âœ… ç—…æ¯’æŠ‘åˆ¶ç‡ {viral_supp:.1f}% è¾¾åˆ°ä¼˜ç§€æ ‡å‡†(â‰¥85%)")
    elif viral_supp >= 70:
        print(f"âš ï¸  ç—…æ¯’æŠ‘åˆ¶ç‡ {viral_supp:.1f}% è¾¾åˆ°åˆæ ¼æ ‡å‡†(70-85%)")
    else:
        print(f"âŒ ç—…æ¯’æŠ‘åˆ¶ç‡ {viral_supp:.1f}% æœªè¾¾æ ‡(<70%)")
    
    if cd4_safe >= 90:
        print(f"âœ… CD4å®‰å…¨ç‡ {cd4_safe:.1f}% è¡¨æ˜å…ç–«åŠŸèƒ½ç»´æŒè‰¯å¥½")
    elif cd4_safe >= 75:
        print(f"âš ï¸  CD4å®‰å…¨ç‡ {cd4_safe:.1f}% éœ€å…³æ³¨å…ç–«æ³¢åŠ¨")
    else:
        print(f"âŒ CD4å®‰å…¨ç‡ {cd4_safe:.1f}% å­˜åœ¨å…ç–«é£é™©")
    
    if switches < 10:
        print(f"âœ… å¹³å‡åˆ‡æ¢ {switches:.1f} æ¬¡ï¼Œæ²»ç–—æ–¹æ¡ˆç¨³å®šæ€§å¥½")
    elif switches < 20:
        print(f"âš ï¸  å¹³å‡åˆ‡æ¢ {switches:.1f} æ¬¡ï¼Œé€‚åº¦è°ƒæ•´")
    else:
        print(f"âŒ å¹³å‡åˆ‡æ¢ {switches:.1f} æ¬¡ï¼Œå¯èƒ½å¢åŠ ä¾ä»æ€§è´Ÿæ‹…")
    
    print("\nã€3. ç›¸å¯¹æ”¹è¿›åˆ†æã€‘")
    print("-" * 70)
    
    for metric, values in metrics_dict.items():
        dqn_val = values[0]
        fixed_val = values[1]
        if fixed_val != 0:
            improvement = (dqn_val - fixed_val) / abs(fixed_val) * 100
            direction = "â†‘" if improvement > 0 else "â†“"
            print(f"{metric}: {direction} {abs(improvement):.1f}% (ç›¸æ¯”å›ºå®šæ–¹æ¡ˆ)")
    
    print("\nã€4. å…³é”®å‘ç°ã€‘")
    print("-" * 70)
    
    # åˆ†æåŠ¨ä½œåå¥½
    all_actions = []
    for actions in our_results['actions']:
        all_actions.extend(actions)
    action_dist = [all_actions.count(i)/len(all_actions)*100 for i in range(4)]
    action_names = ['æ— æ²»ç–—', 'å•è¯', 'åŒè¯', 'ä¸‰è¯è”åˆ']
    dominant_action = action_names[np.argmax(action_dist)]
    
    print(f"â€¢ ç­–ç•¥æœ€å¸¸ä½¿ç”¨: {dominant_action} ({max(action_dist):.1f}%)")
    print(f"â€¢ æ²»ç–—è´Ÿæ‹…å‡è¡¡æ€§: {'è‰¯å¥½' if action_dist[3] < 60 else 'åé«˜'}")
    
    # ç¨³å®šæ€§åˆ†æ
    reward_std = np.std(our_results['episode_rewards'])
    print(f"â€¢ æ€§èƒ½ç¨³å®šæ€§: æ ‡å‡†å·®={reward_std:.2f} ({'ç¨³å®š' if reward_std < 50 else 'æ³¢åŠ¨è¾ƒå¤§'})")
    
    print("\nã€5. ä¸´åºŠåº”ç”¨å»ºè®®ã€‘")
    print("-" * 70)
    print("â€¢ å»ºè®®åœ¨ç—…æ¯’è½½é‡>10,000æ—¶ä½¿ç”¨ä¸‰è¯è”åˆå¿«é€ŸæŠ‘åˆ¶")
    print("â€¢ CD4>500ä¸”ç—…æ¯’<50æ—¶å¯è€ƒè™‘ç®€åŒ–ä¸ºåŒè¯æ–¹æ¡ˆ")
    print("â€¢ æ¯4-8å‘¨è¯„ä¼°ä¸€æ¬¡ï¼Œæ ¹æ®æŒ‡æ ‡åŠ¨æ€è°ƒæ•´")
    print("â€¢ åˆ‡æ¢æ–¹æ¡ˆæ—¶éœ€è¯„ä¼°æ‚£è€…ä¾ä»æ€§å’Œè€å—æ€§")
    
    print("\n" + "="*70)
    print("æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("="*70 + "\n")


# ==================== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šä¸»å‡½æ•° ====================

def main():
    """
    ä¸»æ‰§è¡Œæµç¨‹
    ä¸ºä½•åˆ†æ­¥éª¤ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºè°ƒè¯•å’Œæ‰©å±•
    """
    print("="*70)
    print("ğŸ¥ HIVé•¿æœŸæ²»ç–—ç­–ç•¥ä¼˜åŒ–ç³»ç»Ÿ")
    print("åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ (DQN)çš„ä¸ªæ€§åŒ–æ²»ç–—æ–¹æ¡ˆç”Ÿæˆ")
    print("="*70)
    
    # è®¾ç½®éšæœºç§å­ï¼ˆä¸ºä½•éœ€è¦ï¼šä¿è¯ç»“æœå¯å¤ç°ï¼‰
    np.random.seed(42)
    torch.manual_seed(42)
    random.seed(42)
    
    # ç¬¬1æ­¥ï¼šåˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“
    print("\nã€æ­¥éª¤1ã€‘åˆå§‹åŒ–ç¯å¢ƒå’Œæ™ºèƒ½ä½“...")
    env = HIVTreatmentEnv()
    agent = HIVTreatmentAgent(
        state_dim=env.state_dim,
        action_dim=env.action_dim
    )
    print(f"âœ… ç¯å¢ƒçŠ¶æ€ç»´åº¦: {env.state_dim}")
    print(f"âœ… åŠ¨ä½œç©ºé—´å¤§å°: {env.action_dim}")
    
    # ç¬¬2æ­¥ï¼šè®­ç»ƒæ™ºèƒ½ä½“
    print("\nã€æ­¥éª¤2ã€‘è®­ç»ƒæ™ºèƒ½ä½“...")
    num_train_episodes = 500  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
    history = train_agent(env, agent, num_episodes=num_train_episodes)
    
    # ç¬¬3æ­¥ï¼šå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    print("\nã€æ­¥éª¤3ã€‘å¯è§†åŒ–è®­ç»ƒå†å²...")
    plot_training_history(history)
    
    # ç¬¬4æ­¥ï¼šè¯„ä¼°è®­ç»ƒå¥½çš„ç­–ç•¥
    print("\nã€æ­¥éª¤4ã€‘è¯„ä¼°è®­ç»ƒå¥½çš„ç­–ç•¥...")
    num_eval_episodes = 100
    our_results = evaluate_policy(env, agent, num_episodes=num_eval_episodes)
    
    # ç¬¬5æ­¥ï¼šè¯„ä¼°åŸºçº¿æ–¹æ³•
    print("\nã€æ­¥éª¤5ã€‘è¯„ä¼°åŸºçº¿æ–¹æ³•...")
    print("  - è¯„ä¼°å›ºå®šæ–¹æ¡ˆ...")
    baseline_fixed = evaluate_baseline(env, 'fixed', num_episodes=num_eval_episodes)
    print("  - è¯„ä¼°å¾ªç¯æ–¹æ¡ˆ...")
    baseline_cycling = evaluate_baseline(env, 'cycling', num_episodes=num_eval_episodes)
    
    # ç¬¬6æ­¥ï¼šå¯è§†åŒ–å¯¹æ¯”ç»“æœ
    print("\nã€æ­¥éª¤6ã€‘ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")
    plot_comparison(our_results, baseline_fixed, baseline_cycling)
    
    # ç¬¬7æ­¥ï¼šå¯è§†åŒ–å•ä¸ªæ‚£è€…è½¨è¿¹
    print("\nã€æ­¥éª¤7ã€‘å¯è§†åŒ–å…¸å‹æ‚£è€…æ²»ç–—è½¨è¿¹...")
    plot_treatment_trajectory(our_results, episode_idx=0)
    
    # ç¬¬8æ­¥ï¼šåˆ†æåŠ¨ä½œåˆ†å¸ƒ
    print("\nã€æ­¥éª¤8ã€‘åˆ†ææ²»ç–—æ–¹æ¡ˆé€‰æ‹©åˆ†å¸ƒ...")
    plot_action_distribution(our_results)
    
    # ç¬¬9æ­¥ï¼šç”Ÿæˆç»¼åˆæŠ¥å‘Š
    print("\nã€æ­¥éª¤9ã€‘ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
    generate_report(our_results, baseline_fixed, baseline_cycling)
    
    # ç¬¬10æ­¥ï¼šä¿å­˜æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    print("\nã€æ­¥éª¤10ã€‘ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
    torch.save({
        'policy_net_state_dict': agent.policy_net.state_dict(),
        'target_net_state_dict': agent.target_net.state_dict(),
        'optimizer_state_dict': agent.optimizer.state_dict(),
    }, 'hiv_treatment_model.pth')
    print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: hiv_treatment_model.pth")
    
    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("="*70)
    
    # è¿”å›ç»“æœä¾›è¿›ä¸€æ­¥åˆ†æ
    return {
        'agent': agent,
        'env': env,
        'history': history,
        'our_results': our_results,
        'baseline_fixed': baseline_fixed,
        'baseline_cycling': baseline_cycling
    }


# ==================== æ‰§è¡Œå…¥å£ ====================

if __name__ == "__main__":
    results = main()
    
    # å¯é€‰ï¼šäº¤äº’å¼åˆ†æ
    print("\nğŸ’¡ æç¤ºï¼šä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›ä¸€æ­¥åˆ†æï¼š")
    print("   - results['agent']: è®¿é—®è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“")
    print("   - results['env']: è®¿é—®ç¯å¢ƒ")
    print("   - results['history']: æŸ¥çœ‹è®­ç»ƒå†å²")
    print("   - results['our_results']: æŸ¥çœ‹è¯„ä¼°ç»“æœ")
